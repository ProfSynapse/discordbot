# Implementation Plan: Conversational Memory Pipeline

> Generated by `/PACT:plan-mode` on 2026-01-30
> Status: IMPLEMENTED

## Summary

Build a conversational memory pipeline that detects topic shifts in Discord conversations using **hybrid detection** (heuristics + sentence-transformers embeddings), chunks them at natural topic boundaries, and outputs in **dual format**: ChatML/JSONL for future fine-tuning AND text for GPT Trainer RAG retrieval. Chunks include **Gemini-generated summaries**. The architecture uses a modular package structure with background processing and event-driven uploads with SQLite persistence.

## Resolved Decisions

| Decision | Choice | Details |
|----------|--------|---------|
| Topic detection | **Gemini-powered** | Heuristics (time gaps) + Gemini API for subtle shift detection (no local model) |
| Output format | **Dual: JSONL + RAG** | ChatML/JSONL saved locally for fine-tuning + converted to text for GPT Trainer |
| Channel scope | **Configurable whitelist** | Starting with Town Square (`1147552596228321412`), add via `MEMORY_ENABLED_CHANNELS` env var |
| Summaries | **Gemini-generated** | AI summary per chunk using existing Gemini integration |

---

## Specialist Perspectives

### ðŸ“‹ Preparation Phase
**Effort**: Medium

#### Research Completed
- [x] GPT Trainer API analysis - `upload_data_source()` currently URL-only, file upload endpoint exists but needs testing
- [x] Topic detection approaches evaluated - Gemini API chosen (reuses existing integration)
- [x] Existing Gemini integration reviewed - Already set up in `image_generator.py`

#### Research Still Needed
- [ ] **GPT Trainer text upload verification** - Confirm text/markdown format accepted for RAG

#### Dependencies to Map
- No new external dependencies (Gemini already integrated)
- GPT Trainer upload endpoint

#### Preparation Artifact
Research document created at: `docs/preparation/conversational-memory-pipeline-research.md`

---

### ðŸ—ï¸ Architecture Phase
**Effort**: Medium-High

#### Components Needed

| Component | File | Responsibility |
|-----------|------|----------------|
| ConversationBuffer | `memory/buffer.py` | Per-channel ring buffer for recent messages |
| TopicDetector | `memory/detector.py` | Detects topic shifts via heuristics + Gemini API |
| ChunkPackager | `memory/packager.py` | Generates ChatML/JSONL + text for RAG |
| MemoryUploader | `memory/uploader.py` | Queues and uploads chunks to GPT Trainer |
| Pipeline | `memory/pipeline.py` | Orchestrates the full flow |
| Models | `memory/models.py` | Dataclasses for messages, chunks, metadata |

#### Design Approach

```
Discord Message â†’ Buffer â†’ [Background Task] â†’ Detector â†’ Packager â†’ Uploader â†’ GPT Trainer RAG
                    â†“                                          â†“
                SQLite (checkpoint)                    SQLite (upload queue)
```

**Key patterns:**
- Pipeline pattern for sequential processing
- Producer-consumer for buffer â†’ uploader
- Strategy pattern for swappable topic detection algorithms
- Background async task (every 30-60s) for non-blocking detection

#### Key Decisions

| Decision | Options | Recommendation | Rationale |
|----------|---------|----------------|-----------|
| Component boundaries | Monolithic / Microservice / Modular | **Modular package** | Testable units, single deployment |
| Storage strategy | Memory-only / SQLite-only / Hybrid | **Hybrid** | Fast hot path, durable persistence |
| Topic detection placement | Inline / Background / Batched | **Background** | No message latency impact |
| Upload timing | Real-time / Scheduled / Event-driven | **Event-driven** | Natural boundaries, efficient |

#### Interface Contracts

```python
@dataclass
class ConversationMessage:
    message_id: str
    channel_id: str
    user_id: str
    username: str
    content: str
    timestamp: datetime
    is_bot_response: bool

@dataclass
class TopicShiftResult:
    is_shift: bool
    confidence: float
    topic_summary: Optional[str]

class TopicDetector(Protocol):
    async def detect_shift(self, messages: List[ConversationMessage]) -> TopicShiftResult: ...

class ChunkPackager:
    def package_chunk(self, messages: List[ConversationMessage], summary: str, metadata: ChunkMetadata) -> str: ...
```

#### Database Schema Addition

```sql
CREATE TABLE IF NOT EXISTS conversation_chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    chunk_hash TEXT UNIQUE NOT NULL,
    channel_id TEXT NOT NULL,
    start_time TEXT NOT NULL,
    end_time TEXT NOT NULL,
    message_count INTEGER NOT NULL,
    topic_summary TEXT,
    upload_status TEXT DEFAULT 'pending',  -- pending, uploaded, failed
    created_at TEXT NOT NULL,
    uploaded_at TEXT,
    error_message TEXT
);
```

---

### ðŸ’» Code Phase
**Effort**: Medium-High (~665 lines new, ~50 lines modified)

#### Files to Create

| File | Purpose | Est. Lines |
|------|---------|------------|
| `memory/__init__.py` | Package exports | ~15 |
| `memory/models.py` | Dataclasses (ConversationMessage, ChunkMetadata, etc.) | ~80 |
| `memory/buffer.py` | ConversationBuffer with per-channel deques | ~150 |
| `memory/detector.py` | TopicDetector with heuristics + Gemini API | ~120 |
| `memory/packager.py` | ChunkPackager for JSONL + text generation | ~200 |
| `memory/summarizer.py` | Gemini-based summary generation | ~80 |
| `memory/uploader.py` | MemoryUploader with queue and retry logic | ~120 |
| `memory/pipeline.py` | ConversationMemoryPipeline orchestrator | ~200 |

#### Files to Modify

| File | Changes |
|------|---------|
| `main.py` | Hook `on_message()` to `pipeline.track_message()`, add cleanup in `close()` |
| `config.py` | Add `MemoryConfig` dataclass with enable flag, thresholds, channel whitelist (default: `1147552596228321412`) |
| `api_client.py` | Add `upload_text()` method for text-based RAG upload |

#### Implementation Sequence
1. Create `memory/models.py` with dataclasses
2. Implement `memory/buffer.py` with ConversationBuffer
3. Implement `memory/detector.py` with heuristic detector (embeddings as optional)
4. Implement `memory/packager.py` with markdown + frontmatter generation
5. Implement `memory/uploader.py` with queue and SQLite persistence
6. Implement `memory/pipeline.py` orchestrator
7. Wire into `main.py` and `config.py`
8. Add tests

#### Output Formats

**Primary: ChatML/JSONL (for fine-tuning)**
```jsonl
{
  "messages": [
    {"role": "user", "content": "How do I set up the bot?"},
    {"role": "assistant", "content": "Just run python main.py"},
    {"role": "user", "content": "What about the API keys?"},
    {"role": "assistant", "content": "Add them to your .env file"}
  ],
  "metadata": {
    "chunk_id": "abc123",
    "channel_id": "1147552596228321412",
    "channel_name": "town-square",
    "timestamp_start": "2026-01-30T14:30:00Z",
    "timestamp_end": "2026-01-30T14:35:00Z",
    "participants": ["Alice", "Bot"],
    "message_count": 4
  },
  "reflection": {
    "topic": "Bot Setup and Configuration",
    "what_happened": "Alice needed help getting the bot running for the first time. Straightforward onboarding â€” had the code but unsure how to launch or configure credentials.",
    "key_insights": [
      "New user onboarding follows natural progression: run â†’ configure",
      "Environment variables are a common friction point â€” anticipate this question"
    ],
    "about_the_user": [
      "Appears new to the project",
      "Comfortable with command line basics",
      "May need guidance on which API keys are required"
    ],
    "decisions_made": [
      "Recommended running main.py directly (simplest path)",
      "Pointed to .env for configuration (standard pattern)"
    ],
    "what_went_well": [
      "Quick, clear answers without overwhelming detail",
      "User got unblocked in under 5 minutes"
    ],
    "what_could_improve": [
      "Could have proactively listed which API keys are needed",
      "Could have offered to help verify their setup works"
    ],
    "connections": {
      "related_topics": ["bot installation", "environment setup", "onboarding"],
      "likely_next_questions": ["specific API keys needed", "troubleshooting startup errors"]
    }
  }
}
```

**Secondary: Text (for GPT Trainer RAG upload)**
```markdown
---
type: conversation
topic: Bot Setup and Configuration
channel: town-square
channel_id: "1147552596228321412"
date: 2026-01-30
time_start: "14:30:00"
time_end: "14:35:00"
duration_minutes: 5
participants:
  - Alice
  - Bot
message_count: 4
tags:
  - onboarding
  - setup
  - configuration
  - environment-variables
related:
  - "[[Bot Installation]]"
  - "[[Environment Setup]]"
---

# Bot Setup and Configuration

## Reflection

### What Happened
Alice needed help getting the bot running for the first time. This was a straightforward onboarding conversation â€” they had the code but weren't sure how to launch it or configure credentials.

### Key Insights
- **New user onboarding pattern**: Alice's questions followed a natural progression (run â†’ configure). This suggests our setup docs should lead with "how to run" before diving into configuration.
- **Environment variables are a common friction point**: The follow-up question about API keys came immediately, indicating this is a predictable next step we should anticipate.

### What I Learned About the User
- Alice appears to be new to the project
- They're comfortable with command line basics (understood "run python main.py")
- They may need guidance on which API keys are required

### Decisions Made
- Recommended running `main.py` directly (simplest path)
- Pointed to `.env` for configuration (standard pattern)

### What Went Well
- Quick, clear answers without overwhelming detail
- User got unblocked in under 5 minutes

### What Could Be Improved
- Could have proactively listed which API keys are needed
- Could have offered to help verify their setup works

### Connections
- Related to: bot installation, environment setup, onboarding
- User might next ask about: specific API keys needed, troubleshooting startup errors

---

## Transcript
[14:30:15] Alice: How do I set up the bot?
[14:30:45] Bot: Just run python main.py
[14:31:02] Alice: What about the API keys?
[14:31:15] Bot: Add them to your .env file
```

**File locations:**
- JSONL: `data/conversations/{channel_id}/{date}.jsonl` (append per day)
- Text: Uploaded to GPT Trainer RAG (not stored locally)

---

### ðŸ§ª Test Phase
**Effort**: Medium-High

#### Test Scenarios

| Scenario | Type | Priority |
|----------|------|----------|
| Single-topic conversation â†’ one chunk | Unit/Integration | P0 |
| Multi-topic conversation â†’ multiple chunks | Integration | P0 |
| Topic shift detection accuracy | Unit (golden dataset) | P0 |
| Empty/single message edge cases | Unit | P1 |
| Upload retry on API failure | Integration | P1 |
| Buffer overflow handling | Unit | P1 |
| Markdown formatting correctness | Unit | P2 |
| VCR-recorded API integration | Integration | P2 |

#### Coverage Targets
- Topic detection: 85%+
- Chunk boundary logic: 90%+
- Markdown formatter: 80%+
- Pipeline orchestration: 80%+

#### Test Data Needs
- Golden dataset: 20-30 labeled conversations with expected topic boundaries
- Conversation fixtures: Single-topic, multi-topic, edge cases
- VCR cassettes: GPT Trainer API responses

#### Testing Strategy
- **Hybrid approach**: Small golden dataset (20-30) + property-based tests for edge cases
- **VCR recording** for API integration tests (deterministic, no API costs in CI)
- **Deterministic mode** for topic detector (seedable for reproducibility)

---

## Synthesized Implementation Roadmap

### Phase Sequence

```
PREPARE (research) â†’ ARCHITECT (design) â†’ CODE (implement) â†’ TEST (verify)
       â†“                    â†“                   â†“                â†“
   Verify GPT        Define interfaces     Build modules     Golden dataset
   Trainer upload    and data flow         incrementally     + integration
```

### Commit Sequence (Proposed)

1. `feat(memory): add data models for conversation memory` â€” ConversationMessage, ChunkMetadata, ChatML dataclasses
2. `feat(memory): implement conversation buffer` â€” Per-channel ring buffer with deque
3. `feat(memory): implement Gemini-powered topic detector` â€” Heuristics + Gemini API for shift detection
4. `feat(memory): implement Gemini summarizer` â€” AI-generated chunk summaries
5. `feat(memory): implement chunk packager` â€” ChatML/JSONL + text generation for dual output
6. `feat(memory): implement memory uploader` â€” Queue, SQLite persistence, retry logic, GPT Trainer integration
7. `feat(memory): implement pipeline orchestrator` â€” Wire components together
8. `feat(memory): integrate pipeline into bot` â€” Hook on_message, config with Town Square channel
9. `test(memory): add unit and integration tests` â€” Golden dataset, VCR recordings

---

## Cross-Cutting Concerns

| Concern | Status | Notes |
|---------|--------|-------|
| Security | Ready | No credentials in code; messages already in Discord (no new exposure) |
| Performance | Needs attention | Background processing mitigates latency; monitor memory with embeddings |
| Accessibility | N/A | Backend feature |
| Observability | Needs attention | Add logging for chunk creation, upload status, detection decisions |

---

## Open Questions

### Require User Decision
*All user decisions resolved â€” see "Resolved Decisions" section above.*

### Require Further Research (Addressed in PREPARE Phase)
- [ ] GPT Trainer text upload format verification
- [ ] Optimal Gemini prompt for topic shift detection

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Gemini API cost for detection + summaries | Medium | Medium | Batch detection checks; tune detection interval; monitor usage |
| GPT Trainer text upload issues | Low | Medium | Text format is simpler than file upload; can adjust format |
| Topic detection accuracy is poor | Low | Medium | Gemini is smart; tune prompts; can adjust detection threshold |
| Memory pressure from large buffers | Low | Medium | Bounded deque, max 100 messages per channel |
| API rate limits on uploads | Low | Low | Queue with backoff, existing retry patterns |

---

## Scope Assessment

- **Overall Complexity**: Medium-High
- **Estimated Files**: 8 new, 3 modified
- **Specialists Required**: Preparer, Architect, Backend Coder, Test Engineer
- **External Dependencies**: None new (uses existing Gemini + GPT Trainer APIs)
- **New Env Vars**: `MEMORY_ENABLED_CHANNELS` (default: `1147552596228321412`)

---

## Next Steps

To implement this plan after approval:
```
/PACT:orchestrate Build conversational memory pipeline with topic detection, chunking, and GPT Trainer upload
```

The orchestrator will reference this plan during execution.
